{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2pSSc/v6OSwInc3N/V8lM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-h58POWDAbEA"},"outputs":[],"source":["from torchsummary import summary\n","\n","# Define the MLP model\n","class MLP_HSIC_BP(nn.Module):\n","    def __init__(self):\n","        super(MLP_HSIC_BP, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.layer1 = nn.Linear(28*28, 512)\n","        self.layer2 = nn.Linear(512, 256)\n","        self.layer3 = nn.Linear(256, 128)\n","        self.layer4 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        #x = self.flatten(x)\n","        x = x.view(x.size(0), -1)  # Flatten the input\n","        out1 = torch.relu(self.layer1(x))\n","        out2 = torch.relu(self.layer2(out1))\n","        out3 = torch.relu(self.layer3(out2))\n","        out4 = self.layer4(out3)\n","        return out1, out2, out3, out4\n","\n","# Define the HSIC bottleneck class\n","class HSICBottleneck:\n","    def __init__(self, model, batch_size, lambda_0, sigma, lr=0.001):\n","        self.model = model\n","        self.batch_size = batch_size\n","        self.lambda_0 = lambda_0\n","        self.sigma = sigma\n","        self.lr = lr\n","        self.opt = optim.SGD(model.parameters(), lr)\n","        self.remember = []\n","        self.count = 0\n","\n","    def step(self, input_data, labels):\n","        Kx = kernel_matrix(input_data, self.sigma)\n","        Ky = kernel_matrix(labels, self.sigma)\n","        total_loss = 0.\n","        self.opt.zero_grad()\n","\n","        out1, _, out3, _ = self.model(input_data)\n","\n","        Kz1 = kernel_matrix(out1, self.sigma)\n","        Kz3 = kernel_matrix(out3, self.sigma)\n","\n","        loss1 = HSIC(Kz1, Kx, self.batch_size)\n","        loss3 = HSIC(Kz3, Kx, self.batch_size)\n","        loss = loss1 - self.lambda_0 * loss3\n","\n","        total_loss += loss\n","        total_loss.backward()\n","        self.opt.step()\n","\n","        self.remember.append(total_loss.item())\n","        return total_loss.item()\n","\n","# Initialize the model, HSIC bottleneck, and other components\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_hsic_bp = MLP_HSIC_BP().to(device)\n","summary(model_hsic_bp, input_size=(1,784 ))\n","hsic_bottleneck = HSICBottleneck(model_hsic_bp, batch_size=128, lambda_0=1.0, sigma=1.0, lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 6\n","train_accuracy_history_hsic_bp = []\n","test_accuracy_history_hsic_bp = []\n","train_loss_history_hsic_bp = []\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model_hsic_bp.train()\n","    correct = 0\n","    total = 0\n","    total_loss = 0\n","\n","    for data, targets in train_loader:\n","        data, targets = data.to(device), targets.to(device)\n","\n","        # Update HSIC layers\n","        hsic_bottleneck.step(data, targets)\n","\n","        # Forward pass and standard backpropagation for BP layers\n","        outputs = model_hsic_bp(data)\n","        loss = criterion(outputs[-1], targets)\n","\n","        optimizer_bp = optim.Adam(list(model_hsic_bp.layer2.parameters()) + list(model_hsic_bp.layer4.parameters()), lr=0.001)\n","        optimizer_bp.zero_grad()\n","        loss.backward()\n","        optimizer_bp.step()\n","\n","        total_loss += loss.item()\n","\n","        # Track training accuracy\n","        _, predicted = torch.max(outputs[-1].data, 1)\n","        total += targets.size(0)\n","        correct += (predicted == targets).sum().item()\n","\n","    train_accuracy = 100 * correct / total\n","    train_accuracy_history_hsic_bp.append(train_accuracy)\n","    train_loss_history_hsic_bp.append(total_loss / len(train_loader))\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%')\n","\n","    # Testing loop\n","    model_hsic_bp.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data, targets in test_loader:\n","            data, targets = data.to(device), targets.to(device)\n","            outputs = model_hsic_bp(data)\n","            _, predicted = torch.max(outputs[-1].data, 1)\n","            total += targets.size(0)\n","            correct += (predicted == targets).sum().item()\n","\n","    test_accuracy = 100 * correct / total\n","    test_accuracy_history_hsic_bp.append(test_accuracy)\n","\n","    print(f'Test Accuracy: {test_accuracy:.2f}%')\n"]}]}